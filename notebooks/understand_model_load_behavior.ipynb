{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset conll04\n",
    "\n",
    "This dataset is constructed from news articles and has "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "import json\n",
    "from abc import abstractmethod, ABC\n",
    "from collections import OrderedDict\n",
    "from logging import Logger\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from spert import util, models, prediction,  sampling\n",
    "from spert.entities import Dataset, EntityType, RelationType, Entity, Relation, Document\n",
    "from spert.opt import spacy\n",
    "from spert.evaluator import Evaluator\n",
    "from spert.input_reader import JsonInputReader, BaseInputReader\n",
    "from spert.loss import SpERTLoss, Loss\n",
    "from spert.trainer import BaseTrainer\n",
    "from spert.models import SpERT,SpROB, SpLONG\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from logging import raiseExceptions\n",
    "import math\n",
    "import os\n",
    "from typing import Type\n",
    "from unittest import case\n",
    "\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim import Optimizer\n",
    "import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertConfig,RobertaConfig,LongformerConfig  \n",
    "from transformers import BertTokenizer,RobertaTokenizer,LongformerTokenizer\n",
    "from transformers import BertModel, RobertaModel,LongformerModel\n",
    "from transformers import BertPreTrainedModel, RobertaPreTrainedModel,LongformerPreTrainedModel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities and Relation Types\n",
    "Both types are in file `conll04_types.json` and can be extracted as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short</th>\n",
       "      <th>verbose</th>\n",
       "      <th>symmetric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Work_For</th>\n",
       "      <td>Work</td>\n",
       "      <td>Work for</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kill</th>\n",
       "      <td>Kill</td>\n",
       "      <td>Kill</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OrgBased_In</th>\n",
       "      <td>OrgBI</td>\n",
       "      <td>Organization based in</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Live_In</th>\n",
       "      <td>Live</td>\n",
       "      <td>Live in</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Located_In</th>\n",
       "      <td>LocIn</td>\n",
       "      <td>Located in</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             short                verbose symmetric\n",
       "Work_For      Work               Work for     False\n",
       "Kill          Kill                   Kill     False\n",
       "OrgBased_In  OrgBI  Organization based in     False\n",
       "Live_In       Live                Live in     False\n",
       "Located_In   LocIn             Located in     False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "types_path = \"../data/datasets/conll04/conll04_types.json\"\n",
    "\n",
    "types = json.load(open(types_path), object_pairs_hook=OrderedDict)\n",
    "entity_types = pd.DataFrame(types['entities']).transpose()\n",
    "relation_types = pd.DataFrame(types['relations']).transpose()\n",
    "\n",
    "relation_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short</th>\n",
       "      <th>verbose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Loc</th>\n",
       "      <td>Loc</td>\n",
       "      <td>Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Org</th>\n",
       "      <td>Org</td>\n",
       "      <td>Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Peop</th>\n",
       "      <td>Peop</td>\n",
       "      <td>People</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Other</th>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       short       verbose\n",
       "Loc      Loc      Location\n",
       "Org      Org  Organization\n",
       "Peop    Peop        People\n",
       "Other  Other         Other"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splits\n",
    "\n",
    "The data are pre-split into train, train-dev, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'dev', 'test'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [\"../data/datasets/conll04/conll04_train.json\",\n",
    "         #\"../data/datasets/conll04/conll04_train_dev.json\",\n",
    "         \"../data/datasets/conll04/conll04_dev.json\",\n",
    "         \"../data/datasets/conll04/conll04_test.json\"]\n",
    "#labels = [\"train\",\"train_dev\",\"dev\",\"test\"]\n",
    "labels = [\"train\",\"dev\",\"test\"]\n",
    "data_splits = {}\n",
    "for pth, label in zip(paths,labels):\n",
    "    data_splits[label] = json.load(open(pth), object_pairs_hook=OrderedDict)\n",
    "data_splits.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = data_splits['test'][100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train-dev set is the union of train and dev as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train has 922 examples', 'dev has 231 examples', 'test has 288 examples']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ \"%s has %i examples\"%(l,len(s)) for l,s in data_splits.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size  of each train-dev example measured as token count:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sizes = [len(ex['tokens']) for  ex in data_splits['train']+data_splits['dev'] ]\n",
    "plot_bins = plt.hist(sizes, bins=50)\n",
    "#plot_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq1 = data_splits['train'][200]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MEMPHIS', ',', 'Tenn', '.']\n",
      "{'input_ids': [101, 100, 1010, 100, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [0, 3, 6, 34141, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "bertTok = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=False,cache_dir=None,do_basic_tokenize=False)\n",
    "robTok  = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "print(seq1)\n",
    "print(bertTok.encode_plus(seq1))\n",
    "print(robTok.encode_plus(seq1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing SpERT: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing SpERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['entity_classifier.weight', 'size_embeddings.weight', 'entity_classifier.bias', 'rel_classifier.bias', 'rel_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base 1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing SpROB: ['roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'lm_head.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.pooler.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.embeddings.position_embeddings.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'lm_head.dense.weight', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing SpROB from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SpROB from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of SpROB were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.11.attention.self.query.bias', 'roberta.bert.encoder.layer.1.output.LayerNorm.weight', 'roberta.bert.encoder.layer.6.output.LayerNorm.weight', 'roberta.bert.encoder.layer.3.intermediate.dense.bias', 'roberta.bert.encoder.layer.9.output.dense.bias', 'roberta.bert.encoder.layer.0.output.LayerNorm.weight', 'roberta.bert.encoder.layer.3.output.dense.weight', 'roberta.bert.encoder.layer.10.attention.self.value.bias', 'roberta.bert.encoder.layer.4.output.LayerNorm.bias', 'roberta.bert.encoder.layer.3.output.LayerNorm.weight', 'roberta.bert.encoder.layer.2.attention.self.query.bias', 'roberta.bert.encoder.layer.11.attention.output.dense.weight', 'roberta.bert.encoder.layer.6.attention.output.dense.bias', 'roberta.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.8.attention.output.dense.bias', 'roberta.bert.encoder.layer.1.output.dense.bias', 'roberta.bert.encoder.layer.9.attention.output.dense.weight', 'roberta.bert.encoder.layer.6.attention.self.key.weight', 'roberta.bert.encoder.layer.10.output.LayerNorm.weight', 'roberta.bert.encoder.layer.6.attention.self.query.bias', 'roberta.bert.encoder.layer.2.attention.output.dense.bias', 'roberta.bert.encoder.layer.3.attention.self.key.weight', 'roberta.bert.encoder.layer.9.attention.self.value.weight', 'roberta.bert.encoder.layer.9.intermediate.dense.bias', 'roberta.bert.encoder.layer.5.attention.output.dense.bias', 'roberta.bert.encoder.layer.2.output.dense.bias', 'roberta.bert.encoder.layer.6.intermediate.dense.weight', 'roberta.bert.encoder.layer.6.attention.self.key.bias', 'roberta.bert.encoder.layer.8.attention.output.dense.weight', 'roberta.bert.encoder.layer.2.output.dense.weight', 'roberta.bert.encoder.layer.10.output.dense.weight', 'roberta.bert.encoder.layer.8.intermediate.dense.bias', 'roberta.bert.encoder.layer.11.output.LayerNorm.weight', 'roberta.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.5.attention.self.query.bias', 'roberta.bert.encoder.layer.2.attention.self.value.bias', 'roberta.bert.encoder.layer.10.intermediate.dense.bias', 'roberta.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.11.output.dense.bias', 'roberta.bert.encoder.layer.5.attention.self.query.weight', 'roberta.bert.pooler.dense.weight', 'roberta.bert.encoder.layer.5.attention.self.value.weight', 'roberta.bert.encoder.layer.10.attention.self.value.weight', 'roberta.bert.encoder.layer.3.output.dense.bias', 'roberta.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.0.intermediate.dense.bias', 'roberta.bert.encoder.layer.5.output.dense.weight', 'roberta.bert.encoder.layer.10.attention.output.dense.bias', 'roberta.bert.encoder.layer.4.attention.output.dense.bias', 'roberta.bert.encoder.layer.1.attention.self.key.bias', 'roberta.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.3.attention.self.query.weight', 'roberta.bert.encoder.layer.3.attention.output.dense.bias', 'roberta.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.5.attention.self.key.weight', 'roberta.bert.encoder.layer.5.output.LayerNorm.weight', 'roberta.bert.embeddings.token_type_embeddings.weight', 'roberta.bert.encoder.layer.2.attention.self.value.weight', 'roberta.bert.encoder.layer.2.intermediate.dense.weight', 'roberta.bert.encoder.layer.9.intermediate.dense.weight', 'roberta.bert.encoder.layer.0.attention.self.key.weight', 'roberta.bert.encoder.layer.2.attention.self.key.weight', 'roberta.bert.encoder.layer.10.attention.self.query.bias', 'roberta.bert.encoder.layer.1.attention.self.value.bias', 'roberta.bert.encoder.layer.7.attention.self.query.weight', 'roberta.bert.encoder.layer.7.attention.self.query.bias', 'roberta.bert.encoder.layer.6.output.LayerNorm.bias', 'roberta.bert.encoder.layer.8.output.LayerNorm.weight', 'roberta.bert.encoder.layer.9.attention.self.query.bias', 'roberta.bert.encoder.layer.2.attention.output.dense.weight', 'roberta.bert.encoder.layer.1.output.LayerNorm.bias', 'roberta.bert.encoder.layer.6.intermediate.dense.bias', 'roberta.bert.pooler.dense.bias', 'roberta.bert.encoder.layer.6.attention.self.value.bias', 'roberta.bert.encoder.layer.4.attention.self.query.weight', 'roberta.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.entity_classifier.weight', 'roberta.bert.encoder.layer.8.output.dense.bias', 'roberta.bert.encoder.layer.3.attention.output.dense.weight', 'roberta.bert.encoder.layer.2.output.LayerNorm.weight', 'roberta.bert.encoder.layer.7.output.dense.bias', 'roberta.bert.encoder.layer.1.attention.self.value.weight', 'roberta.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.4.output.dense.bias', 'roberta.bert.encoder.layer.7.attention.output.dense.bias', 'roberta.rel_classifier.bias', 'roberta.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.bert.embeddings.position_ids', 'roberta.bert.encoder.layer.0.attention.output.dense.weight', 'roberta.bert.encoder.layer.8.output.dense.weight', 'roberta.bert.encoder.layer.9.attention.self.query.weight', 'roberta.bert.encoder.layer.1.attention.self.key.weight', 'roberta.bert.encoder.layer.7.attention.self.key.bias', 'roberta.bert.embeddings.LayerNorm.bias', 'roberta.bert.encoder.layer.7.intermediate.dense.bias', 'roberta.bert.encoder.layer.1.attention.output.dense.weight', 'roberta.bert.encoder.layer.7.attention.self.key.weight', 'roberta.bert.encoder.layer.8.output.LayerNorm.bias', 'roberta.bert.embeddings.LayerNorm.weight', 'roberta.bert.encoder.layer.0.attention.self.value.bias', 'roberta.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.4.output.LayerNorm.weight', 'roberta.bert.encoder.layer.1.attention.self.query.weight', 'roberta.bert.encoder.layer.5.attention.output.dense.weight', 'roberta.entity_classifier.bias', 'roberta.bert.encoder.layer.11.attention.self.key.weight', 'roberta.bert.encoder.layer.8.attention.self.key.bias', 'roberta.bert.encoder.layer.5.intermediate.dense.weight', 'roberta.bert.encoder.layer.9.attention.self.value.bias', 'roberta.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.10.output.LayerNorm.bias', 'roberta.bert.encoder.layer.2.output.LayerNorm.bias', 'roberta.bert.encoder.layer.0.output.dense.bias', 'roberta.bert.encoder.layer.4.attention.self.value.weight', 'roberta.bert.encoder.layer.11.intermediate.dense.weight', 'roberta.bert.encoder.layer.9.output.LayerNorm.bias', 'roberta.bert.encoder.layer.11.intermediate.dense.bias', 'roberta.bert.encoder.layer.0.intermediate.dense.weight', 'roberta.bert.encoder.layer.0.output.LayerNorm.bias', 'roberta.bert.encoder.layer.3.intermediate.dense.weight', 'roberta.bert.encoder.layer.10.attention.output.dense.weight', 'roberta.bert.encoder.layer.2.attention.self.query.weight', 'roberta.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.0.attention.self.query.bias', 'roberta.bert.embeddings.position_embeddings.weight', 'roberta.bert.encoder.layer.3.output.LayerNorm.bias', 'roberta.bert.encoder.layer.2.intermediate.dense.bias', 'roberta.bert.encoder.layer.6.output.dense.bias', 'roberta.bert.encoder.layer.11.attention.output.dense.bias', 'roberta.bert.encoder.layer.4.attention.self.key.weight', 'roberta.bert.encoder.layer.9.attention.output.dense.bias', 'roberta.bert.encoder.layer.11.output.dense.weight', 'roberta.bert.encoder.layer.9.attention.self.key.bias', 'roberta.bert.encoder.layer.6.attention.output.dense.weight', 'roberta.bert.encoder.layer.7.attention.self.value.weight', 'roberta.bert.encoder.layer.5.attention.self.key.bias', 'roberta.bert.encoder.layer.8.attention.self.query.bias', 'roberta.bert.encoder.layer.0.output.dense.weight', 'roberta.bert.encoder.layer.5.attention.self.value.bias', 'roberta.bert.encoder.layer.0.attention.self.key.bias', 'roberta.bert.encoder.layer.7.attention.output.dense.weight', 'roberta.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.4.attention.self.value.bias', 'roberta.bert.encoder.layer.3.attention.self.key.bias', 'roberta.bert.encoder.layer.7.output.dense.weight', 'roberta.bert.encoder.layer.9.output.dense.weight', 'roberta.bert.encoder.layer.10.attention.self.key.weight', 'roberta.bert.encoder.layer.11.attention.self.value.bias', 'roberta.bert.encoder.layer.8.attention.self.value.weight', 'roberta.bert.encoder.layer.11.attention.self.key.bias', 'roberta.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.10.attention.self.key.bias', 'roberta.bert.encoder.layer.4.attention.self.key.bias', 'roberta.bert.encoder.layer.8.intermediate.dense.weight', 'roberta.bert.encoder.layer.3.attention.self.value.weight', 'roberta.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.6.attention.self.value.weight', 'roberta.bert.encoder.layer.2.attention.self.key.bias', 'roberta.bert.encoder.layer.5.intermediate.dense.bias', 'roberta.bert.encoder.layer.9.output.LayerNorm.weight', 'roberta.bert.embeddings.word_embeddings.weight', 'roberta.bert.encoder.layer.10.attention.self.query.weight', 'roberta.bert.encoder.layer.4.attention.output.dense.weight', 'roberta.bert.encoder.layer.4.intermediate.dense.weight', 'roberta.bert.encoder.layer.4.output.dense.weight', 'roberta.bert.encoder.layer.4.attention.self.query.bias', 'roberta.bert.encoder.layer.3.attention.self.query.bias', 'roberta.bert.encoder.layer.7.output.LayerNorm.weight', 'roberta.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.6.output.dense.weight', 'roberta.bert.encoder.layer.1.attention.output.dense.bias', 'roberta.bert.encoder.layer.10.output.dense.bias', 'roberta.bert.encoder.layer.7.attention.self.value.bias', 'roberta.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.bert.encoder.layer.9.attention.self.key.weight', 'roberta.rel_classifier.weight', 'roberta.size_embeddings.weight', 'roberta.bert.encoder.layer.5.output.LayerNorm.bias', 'roberta.bert.encoder.layer.1.intermediate.dense.weight', 'roberta.bert.encoder.layer.8.attention.self.key.weight', 'roberta.bert.encoder.layer.11.attention.self.query.weight', 'roberta.bert.encoder.layer.7.intermediate.dense.weight', 'roberta.bert.encoder.layer.11.output.LayerNorm.bias', 'roberta.bert.encoder.layer.1.intermediate.dense.bias', 'roberta.bert.encoder.layer.0.attention.self.value.weight', 'roberta.bert.encoder.layer.0.attention.self.query.weight', 'roberta.bert.encoder.layer.8.attention.self.value.bias', 'roberta.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.bert.encoder.layer.1.attention.self.query.bias', 'roberta.bert.encoder.layer.1.output.dense.weight', 'roberta.bert.encoder.layer.8.attention.self.query.weight', 'roberta.bert.encoder.layer.11.attention.self.value.weight', 'roberta.bert.encoder.layer.6.attention.self.query.weight', 'roberta.bert.encoder.layer.0.attention.output.dense.bias', 'roberta.bert.encoder.layer.10.intermediate.dense.weight', 'roberta.bert.encoder.layer.5.output.dense.bias', 'roberta.bert.encoder.layer.7.output.LayerNorm.bias', 'roberta.bert.encoder.layer.4.intermediate.dense.bias', 'roberta.bert.encoder.layer.3.attention.self.value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "mtypes = ['spert','sprob']\n",
    "model_names = []\n",
    "configs = []\n",
    "tokenizers = []\n",
    "spmodels = []\n",
    "for mtype in mtypes:\n",
    "    model_class = models.get_model(mtype)\n",
    "\n",
    "    if mtype == 'spert':\n",
    "        model_name = 'bert-base-cased'\n",
    "        config = BertConfig.from_pretrained(model_name, cache_dir=None)\n",
    "        tokenizer = BertTokenizer.from_pretrained(model_name,do_lower_case=False,cache_dir=None)\n",
    "    elif mtype == 'sprob':\n",
    "        model_name = 'roberta-base'\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_name,do_lower_case=False,cache_dir=None)\n",
    "        config = RobertaConfig.from_pretrained(model_name, cache_dir=None)\n",
    "    elif mtype == 'splong':\n",
    "        model_name = 'allenai/longformer-base-4096'\n",
    "        tokenizer = LongformerTokenizer.from_pretrained(model_name,do_lower_case=False,cache_dir=None)\n",
    "        config = LongformerConfig.from_pretrained(model_name, cache_dir=None)\n",
    "    else:\n",
    "        raise Exception(\"Argument model_type must be one of %s\"%['spert','sprob','splong'])\n",
    "\n",
    "    util.check_version(config, model_class, model_name)\n",
    "    config.spert_version = model_class.VERSION\n",
    "    print(model_name,config.spert_version)\n",
    "    model = model_class.from_pretrained(model_name,\n",
    "                                    config=config,\n",
    "                                    # SpERT model parameters\n",
    "                                    #cls_token=self._tokenizer.convert_tokens_to_ids('[CLS]'),\n",
    "                                    cls_token=tokenizer.cls_token_id,\n",
    "                                    relation_types=4,\n",
    "                                    entity_types=5,\n",
    "                                    max_pairs=1000,\n",
    "                                    prop_drop=0.1,\n",
    "                                    size_embedding=25,\n",
    "                                    freeze_transformer=False,\n",
    "                                    cache_dir=None\n",
    "                                    )\n",
    "\n",
    "    model_names.append(model_name)\n",
    "    tokenizers.append(tokenizer)\n",
    "    configs.append(config)\n",
    "    spmodels.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_name = 'bert-base-cased'\n",
    "roberta_name = 'roberta-base'\n",
    "bert_config = BertConfig.from_pretrained(bert_name, cache_dir=None)\n",
    "roberta_config = RobertaConfig.from_pretrained(roberta_name, cache_dir=None)\n",
    "\n",
    "bert_model = BertModel.from_pretrained(bert_name, config=bert_config)\n",
    "roberta_model = RobertaModel.from_pretrained(roberta_name,config=roberta_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.encoder.layer[8]\n",
    "bert_model.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0265,  0.0737, -0.0082,  ..., -0.0157, -0.0316,  0.0314],\n",
      "        [ 0.0321, -0.0354, -0.0954,  ..., -0.0011, -0.0240, -0.0122],\n",
      "        [-0.0243,  0.0090, -0.0421,  ...,  0.0460, -0.0162,  0.0854],\n",
      "        ...,\n",
      "        [ 0.0386,  0.0914,  0.0159,  ..., -0.0410,  0.0346,  0.0068],\n",
      "        [ 0.0914,  0.0981,  0.0704,  ...,  0.0023, -0.0318, -0.1753],\n",
      "        [ 0.0071, -0.0322,  0.0246,  ..., -0.0684,  0.0122, -0.1757]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 3.7750e-02,  3.2135e-02, -1.2207e-01,  9.4604e-02, -1.7109e-03,\n",
      "         6.4758e-02,  9.3506e-02,  5.4108e-02, -2.3773e-02, -5.9357e-02,\n",
      "        -1.8280e-02, -3.1677e-02,  3.8013e-01,  1.0252e-03,  4.6356e-02,\n",
      "         2.5711e-02, -3.3844e-02, -1.9608e-02,  7.9651e-02, -3.7079e-02,\n",
      "        -9.8816e-02, -8.4656e-02,  1.2421e-01,  8.5327e-02, -1.2779e-03,\n",
      "         6.8115e-02, -5.6580e-02,  1.7929e-02, -3.2013e-02, -1.5430e-01,\n",
      "        -2.6880e-01, -1.3281e-01, -2.6733e-02,  6.0822e-02, -4.4281e-02,\n",
      "         9.3994e-02,  9.1736e-02, -1.1572e-01, -8.9478e-02, -9.6680e-02,\n",
      "         6.9946e-02, -6.8932e-03,  8.0444e-02, -7.4097e-02, -8.3130e-02,\n",
      "        -1.8848e-01, -3.3203e-02,  8.1825e-04, -1.9379e-02,  2.0294e-02,\n",
      "        -2.6025e-01, -1.4819e-01,  1.1401e-01, -1.6953e-02,  1.0345e-01,\n",
      "        -9.9487e-02, -6.4812e-03, -7.9834e-02,  5.9448e-02,  1.5015e-01,\n",
      "        -4.0344e-02,  6.0577e-02, -1.8494e-02,  5.0354e-02,  2.8000e-02,\n",
      "         5.6244e-02, -1.4641e-02,  5.5267e-02,  1.3115e-02,  1.0419e-01,\n",
      "        -1.0175e-01, -2.2705e-02, -1.4524e-03,  4.1565e-02, -2.5558e-02,\n",
      "         4.1626e-01, -1.7993e-01,  8.4534e-02,  3.9558e-03,  6.1432e-02,\n",
      "        -7.7332e-02,  2.6291e-02, -3.1433e-02, -1.1224e-01,  5.6030e-02,\n",
      "         1.2610e-01,  1.8097e-02,  9.5276e-02,  3.9764e-02,  4.5967e-03,\n",
      "        -1.0521e-02,  7.1068e-03, -1.8372e-02,  7.4829e-02, -2.5131e-02,\n",
      "        -1.1993e-01,  6.0150e-02, -8.8501e-02,  5.4749e-02,  9.1248e-03,\n",
      "        -5.3825e-03, -1.0071e-02, -3.6224e-02,  1.0376e-02,  5.2612e-02,\n",
      "         2.5375e-02, -3.6865e-01, -4.5349e-02,  4.4647e-02, -1.7151e-02,\n",
      "         3.8330e-01, -3.7445e-02, -5.6458e-02,  4.1534e-02,  4.7531e-03,\n",
      "        -7.1411e-02, -1.1711e-02,  4.6448e-02, -1.1700e-01, -7.2571e-02,\n",
      "         5.6244e-02, -4.9927e-02,  2.2766e-02, -7.0984e-02,  4.3213e-02,\n",
      "         3.9795e-02,  3.2837e-02, -3.4302e-02, -3.7720e-01, -7.8064e-02,\n",
      "        -1.1218e-01, -6.5186e-02,  8.2642e-02,  2.3401e-01, -4.5959e-02,\n",
      "         2.2980e-02, -1.0889e-01, -8.4778e-02,  3.9032e-02,  1.1945e-01,\n",
      "        -1.0181e-01, -8.3618e-02, -4.9316e-02, -1.1401e-01, -1.5717e-02,\n",
      "         3.6469e-02, -4.4525e-02,  1.1116e-02,  1.0681e-01, -1.1459e-02,\n",
      "         4.2389e-02, -1.0504e-01, -9.9304e-02,  5.5328e-02, -8.5632e-02,\n",
      "         1.1230e-02, -6.6345e-02, -1.7688e-01, -1.9092e-01,  1.1072e-01,\n",
      "        -3.0960e-02,  1.2482e-01, -1.2335e-01, -4.5288e-02, -1.0785e-01,\n",
      "        -5.0812e-03,  2.6343e-01,  9.3567e-02,  5.6122e-02, -9.1675e-02,\n",
      "         8.4167e-02,  8.1406e-03,  7.1793e-03,  1.1420e-01, -1.0700e-03,\n",
      "         9.3933e-02, -5.0171e-02,  1.2627e-02,  1.9547e-02, -6.2408e-02,\n",
      "         4.3144e-03, -1.0797e-01,  1.6968e-02, -2.8662e-01, -1.6281e-02,\n",
      "         1.2672e-02,  6.3110e-02,  6.4774e-03, -3.7445e-02, -6.2225e-02,\n",
      "        -1.4172e-01,  5.6580e-02, -1.3318e-01,  1.3588e-02, -1.5411e-02,\n",
      "        -1.2158e-01,  9.7168e-02,  5.7983e-02, -1.0339e-01, -1.0658e-02,\n",
      "         1.4392e-01,  8.0627e-02,  8.0627e-02,  1.4473e-02,  1.2006e-01,\n",
      "        -9.7473e-02,  1.7334e-01, -3.6353e-01, -2.3230e-01, -2.9297e-02,\n",
      "        -8.1726e-02, -1.7505e-01, -8.3237e-03, -1.2408e-01,  1.0931e-01,\n",
      "        -1.2036e-01,  1.5297e-02, -6.5735e-02,  2.7679e-02,  3.7903e-02,\n",
      "         2.4829e-01,  5.0720e-02, -7.1106e-02, -1.6772e-01, -1.3220e-01,\n",
      "         2.9468e-01, -1.8933e-01,  1.0994e-02,  1.3257e-01, -9.0088e-02,\n",
      "        -7.8674e-02,  1.1938e-01, -6.5063e-02,  5.2094e-02,  1.0022e-01,\n",
      "        -1.0455e-01, -5.4901e-02,  4.7363e-02,  1.8909e-01,  1.1328e-01,\n",
      "         1.6479e-01, -1.4355e-01,  5.9462e-04,  5.6305e-02,  9.9670e-02,\n",
      "         1.1539e-03, -5.0049e-02, -1.7554e-01,  2.2559e-01, -8.4534e-02,\n",
      "         2.2620e-01,  4.2206e-02, -6.9031e-02, -2.8610e-02,  1.7593e-02,\n",
      "         2.3267e-01,  1.0559e-01,  1.2500e-01,  4.1695e-03,  1.3489e-01,\n",
      "         5.8899e-02, -9.7168e-02,  9.2102e-02,  3.5229e-03, -1.0913e-01,\n",
      "         7.9895e-02,  1.0059e-01,  6.0120e-03,  2.6562e-01,  5.9692e-02,\n",
      "         7.3486e-02,  1.0754e-01, -2.5650e-02,  1.9943e-02, -7.4951e-02,\n",
      "        -6.6185e-03, -2.0398e-01,  6.1462e-02, -2.6154e-02,  1.9263e-01,\n",
      "         3.0197e-02,  4.2725e-01,  2.4170e-01,  3.7915e-01,  2.6352e-02,\n",
      "        -6.9519e-02, -5.6000e-02,  9.9548e-02,  1.9348e-01,  1.9180e-02,\n",
      "        -1.8542e-01,  5.5115e-02,  2.6782e-01,  7.0679e-02,  1.1871e-01,\n",
      "        -2.9556e-02, -1.0022e-01, -5.1941e-02,  3.0640e-02,  1.1316e-01,\n",
      "         6.6589e-02,  5.9662e-02, -5.6213e-02,  1.2335e-01, -3.0786e-01,\n",
      "        -3.5278e-02, -1.8053e-03, -1.0422e-02,  9.1858e-02, -7.2136e-03,\n",
      "         9.7473e-02, -1.6870e-01, -6.0577e-02, -7.5054e-04,  5.4138e-02,\n",
      "        -2.0294e-02,  3.9558e-03,  7.6538e-02, -1.1835e-01,  7.2998e-02,\n",
      "        -1.4145e-02, -5.0079e-02, -1.7563e-02, -6.0150e-02,  8.8074e-02,\n",
      "         1.1414e-01, -6.3782e-02,  1.9318e-02, -6.6956e-02,  3.0258e-02,\n",
      "        -1.6077e-01,  1.4124e-01,  7.3195e-04,  9.4849e-02,  1.9577e-02,\n",
      "        -3.6743e-02,  2.9321e-01,  8.2932e-03, -9.3018e-02,  4.0126e-04,\n",
      "         2.6099e-01, -9.1614e-02, -4.1931e-02, -1.5503e-01,  9.5940e-04,\n",
      "        -2.2620e-01, -9.3384e-02,  8.3008e-02,  1.4185e-01, -8.9844e-02,\n",
      "         1.1005e-01,  1.2146e-01, -8.8577e-03,  9.8572e-03,  6.9824e-02,\n",
      "         3.2324e-01, -8.1848e-02,  3.0493e-01, -2.4353e-01, -8.5144e-02,\n",
      "        -4.9805e-02, -1.3313e-02,  5.0079e-02, -2.2110e-02,  2.0203e-02,\n",
      "         9.0393e-02,  3.1738e-02, -8.0505e-02, -1.1002e-02, -2.5406e-03,\n",
      "        -8.2214e-02,  3.4027e-02,  1.1316e-01,  3.3142e-02, -7.3280e-03,\n",
      "         7.3059e-02, -1.6050e-03, -1.1115e-01,  5.5420e-02, -4.1595e-02,\n",
      "        -1.0016e-01,  7.0801e-02,  5.5313e-03, -3.4302e-02, -1.1310e-01,\n",
      "         2.2476e-02,  4.3396e-02,  5.4565e-02,  9.7656e-02, -5.0888e-03,\n",
      "         2.1765e-01,  2.0248e-02, -5.5115e-02, -2.4878e-01,  1.2360e-03,\n",
      "         6.5674e-02,  7.7915e-04, -7.3486e-02, -9.6893e-03, -2.9907e-02,\n",
      "        -2.4597e-02, -6.1127e-02, -7.4829e-02,  8.0688e-02,  1.1890e-01,\n",
      "         9.4910e-02,  5.6030e-02, -2.5085e-02, -2.1912e-02,  3.2617e-01,\n",
      "         7.9346e-02, -5.3864e-02, -9.5398e-02, -4.1656e-03, -1.0484e-04,\n",
      "        -5.3864e-02,  4.1351e-03,  1.7017e-01,  5.7312e-02, -2.3520e-04,\n",
      "        -8.8196e-02, -1.0901e-01,  3.5767e-01, -1.0742e-02, -4.2877e-02,\n",
      "        -9.8145e-02,  4.2297e-02, -1.0757e-02,  9.8022e-02, -4.5197e-02,\n",
      "         1.1346e-01,  1.9714e-01, -5.5481e-02, -1.6748e-01,  1.1847e-01,\n",
      "         1.0706e-01,  2.9236e-02, -7.8796e-02,  2.7405e-02,  6.6681e-03,\n",
      "         6.3477e-02, -4.0161e-02,  4.7546e-02,  1.7529e-03,  9.3994e-02,\n",
      "        -1.8234e-02, -6.2408e-02, -1.2988e-01, -6.8115e-02,  2.1826e-01,\n",
      "        -6.7200e-02,  1.7883e-01, -1.7349e-02, -7.9529e-02,  2.5171e-01,\n",
      "        -5.4779e-02, -6.2378e-02,  1.5198e-01, -1.8341e-02,  2.2546e-01,\n",
      "        -2.7695e-02, -1.2646e-01,  1.5125e-01,  2.8030e-02, -7.7637e-02,\n",
      "        -2.1143e-01,  7.1983e-03, -6.1218e-02, -3.8452e-02,  1.5112e-01,\n",
      "         9.2468e-02, -1.7456e-01, -8.4534e-02, -9.5215e-02,  5.9174e-02,\n",
      "        -4.6417e-02, -7.2205e-02, -5.6244e-02,  1.1951e-01, -2.7252e-02,\n",
      "        -4.9500e-02, -4.9866e-02, -1.8420e-01, -9.5032e-02, -3.5797e-02,\n",
      "        -9.1675e-02,  2.7790e-03, -4.7272e-02,  8.3252e-02, -2.7026e-01,\n",
      "         1.1877e-01, -5.0781e-02, -1.2360e-01,  1.2463e-01,  1.7578e-01,\n",
      "        -3.5324e-03,  1.7053e-01, -1.4526e-01,  1.1542e-01, -6.9275e-02,\n",
      "         2.8369e-01,  1.1765e-02, -1.7957e-01, -6.2439e-02,  3.9429e-02,\n",
      "         8.1055e-02,  1.4050e-01,  2.6343e-01,  1.9028e-02, -8.0811e-02,\n",
      "         5.0354e-02, -2.4487e-01,  1.6675e-01, -2.5610e-01, -1.9336e-01,\n",
      "        -1.8539e-02, -6.3400e-03,  6.4392e-02,  1.7505e-01, -2.3096e-01,\n",
      "         1.0840e-01, -2.1960e-01,  1.3252e-02, -1.6589e-01,  2.9614e-01,\n",
      "        -2.2095e-01, -2.4683e-01,  1.8921e-01, -1.0513e-02, -2.6489e-01,\n",
      "         1.6052e-01, -1.5613e-01,  1.8097e-02,  2.4622e-01,  2.9370e-01,\n",
      "        -1.3611e-01, -1.6455e-01, -6.2256e-02,  1.7664e-01,  1.9775e-01,\n",
      "        -1.0553e-01, -2.0599e-02,  1.9543e-01, -2.4744e-01, -8.0383e-02,\n",
      "        -1.3391e-01, -4.7583e-01, -1.7578e-01, -9.2834e-02,  2.7752e-03,\n",
      "         5.2490e-02, -4.0985e-02,  1.1070e-02,  2.0312e-01,  1.0278e-01,\n",
      "        -9.7900e-02,  5.4321e-02,  1.3379e-01, -1.2341e-01,  5.9906e-02,\n",
      "        -1.1261e-02, -1.4307e-01, -4.9951e-01,  1.8677e-02, -2.5830e-01,\n",
      "         1.8237e-01,  3.6475e-01, -1.8967e-02,  3.2593e-01, -7.8430e-02,\n",
      "        -1.2079e-01, -6.9397e-02, -1.0693e-01,  8.9661e-02, -2.6291e-02,\n",
      "        -9.1797e-02,  8.0139e-02, -7.0312e-02,  3.9429e-02,  2.7832e-01,\n",
      "        -3.6407e-02, -2.5122e-01,  3.2013e-02, -1.5308e-01, -7.6981e-03,\n",
      "        -5.6396e-02, -3.9551e-02, -4.9438e-02,  8.6792e-02,  1.7761e-02,\n",
      "         5.0598e-02,  2.5742e-02,  3.1952e-02, -1.1761e-01, -1.8997e-02,\n",
      "         2.7863e-02,  1.0614e-01,  7.5378e-02,  4.3732e-02, -9.8324e-04,\n",
      "         1.2825e-02,  9.1400e-03,  1.1969e-01,  4.2267e-02,  2.5833e-02,\n",
      "        -2.4216e-02, -2.6474e-03, -1.2311e-01,  7.9834e-02, -1.0608e-01,\n",
      "        -2.2720e-02, -2.8275e-02, -6.5918e-02, -9.5444e-03, -1.5710e-01,\n",
      "        -1.1499e-01,  1.7776e-02,  5.7404e-02,  1.2988e-01,  3.9703e-02,\n",
      "         2.6443e-02, -4.4365e-03, -2.4033e-03, -1.8311e-03, -4.8370e-02,\n",
      "         1.2164e-01, -5.5328e-02,  5.4718e-02,  2.6413e-02, -2.7008e-03,\n",
      "        -4.1351e-02, -6.0760e-02,  5.1392e-02,  1.1969e-01,  1.0608e-01,\n",
      "         4.4281e-02, -1.7654e-02, -1.2360e-01, -1.4893e-01,  3.0685e-02,\n",
      "         2.7759e-01, -1.0002e-02, -2.6108e-02, -1.2581e-02,  1.2589e-02,\n",
      "         7.5012e-02,  1.6174e-03,  6.8298e-02,  4.7989e-03,  1.0216e-02,\n",
      "        -2.6831e-01, -1.8387e-02,  2.1469e-02, -4.8920e-02, -2.0599e-02,\n",
      "         5.4626e-03,  3.6957e-02,  8.0627e-02,  4.1604e-04, -3.6224e-02,\n",
      "        -1.3748e-02, -1.4706e-03,  6.7871e-02,  5.1003e-03,  6.5956e-03,\n",
      "        -1.0101e-01, -1.7176e-03,  2.3575e-02, -6.8016e-03,  5.9013e-03,\n",
      "        -7.8796e-02, -5.7487e-03, -5.6305e-03,  3.2135e-02, -1.9140e-03,\n",
      "         5.2307e-02,  1.4702e-02, -7.6904e-02, -2.8824e-02,  1.1549e-03,\n",
      "         1.7426e-02,  6.2866e-02, -2.4338e-02, -3.0701e-02, -3.4912e-02,\n",
      "         2.8030e-02,  4.8492e-02, -1.4458e-02, -1.7639e-02, -5.0110e-02,\n",
      "        -9.4971e-02, -2.9492e-01, -8.5907e-03,  2.5978e-03, -7.4951e-02,\n",
      "         7.4501e-03,  3.1128e-02, -1.8600e-02, -4.3701e-02, -1.7273e-02,\n",
      "         1.6138e-01,  2.1790e-02,  3.2616e-03,  1.5404e-02, -3.5400e-02,\n",
      "        -5.6091e-02,  3.0319e-02,  5.0232e-02,  5.3619e-02,  1.6571e-02,\n",
      "        -2.6215e-02,  1.3542e-02, -5.2307e-02, -7.0251e-02, -6.4941e-02,\n",
      "         2.3994e-03,  2.7634e-02,  8.3069e-02, -4.4708e-02, -3.3936e-02,\n",
      "         3.2921e-03, -1.8692e-02, -1.4465e-02, -7.9041e-02,  7.5867e-02,\n",
      "        -1.7899e-02, -8.1055e-02,  2.6703e-02,  8.1543e-02, -1.2077e-02,\n",
      "        -1.0445e-02, -4.8798e-02,  1.1414e-02, -8.6792e-02,  3.4009e-01,\n",
      "         2.5208e-02, -4.4922e-02, -1.8692e-02,  3.5742e-01,  1.7380e-02,\n",
      "         3.8147e-02, -1.8280e-02, -4.1565e-02, -2.5253e-02,  8.8013e-02,\n",
      "         9.6191e-02, -3.4943e-02, -1.1108e-02,  8.3130e-02,  1.0431e-01,\n",
      "        -1.5366e-02,  7.2876e-02, -9.8801e-03, -2.0874e-02,  7.0557e-02,\n",
      "        -1.3885e-03, -5.9662e-02,  9.4238e-02,  2.2003e-02,  7.1716e-02,\n",
      "        -2.3193e-02, -3.2715e-02,  2.4817e-01,  3.5400e-02,  1.6129e-02,\n",
      "        -1.2354e-01,  2.9037e-02, -1.1032e-02], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x in roberta_model.encoder.layer[8].attention.self.query.parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(spmodels[0].bert.encoder.layer[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaLayer(\n",
      "  (attention): RobertaAttention(\n",
      "    (self): RobertaSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (output): RobertaSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): RobertaIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): RobertaOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.0040,  0.0210, -0.0024,  ...,  0.0039, -0.0284, -0.0032],\n",
      "        [-0.0113,  0.0101,  0.0118,  ..., -0.0015, -0.0208,  0.0087],\n",
      "        [-0.0134, -0.0153, -0.0079,  ...,  0.0010,  0.0241,  0.0111],\n",
      "        ...,\n",
      "        [ 0.0208,  0.0286,  0.0132,  ..., -0.0110,  0.0184, -0.0081],\n",
      "        [ 0.0024,  0.0095, -0.0085,  ...,  0.0072,  0.0019, -0.0121],\n",
      "        [ 0.0002,  0.0004,  0.0158,  ..., -0.0198, -0.0075,  0.0052]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#model.parameters().__dir__()\n",
    "print(spmodels[1].bert.encoder.layer[8])\n",
    "\n",
    "for x in spmodels[1].bert.encoder.layer[8].attention.self.query.parameters():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07e16bd5aae80a5a2fd0a0d3b13861e825cae1facac225cc594d80ae9e64367b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
